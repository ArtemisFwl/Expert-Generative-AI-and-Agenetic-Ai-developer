# Ollama + Streamlit Local LLM App

A simple Streamlit web application that connects to a locally running
Ollama model and allows users to interact with it via a browser UI.

## ðŸš€ Features
- Local LLM chat using Ollama
- Streamlit-based web interface
- No API keys required
- Runs fully offline
- Lightweight model support (Gemma, DeepSeek, etc.)

## ðŸ§± Tech Stack
- Python 3.10+
- Streamlit
- Ollama (local LLM server)

## ðŸ“¦ Prerequisites
- Python 3.10 or above
- Ollama installed and running  
  https://ollama.com

## ðŸ“¥ Installation

Clone the repository:
```bash
git clone https://github.com/your-username/ollama-with-streamlit.git
cd ollama-with-streamlit
